{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first web scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, how will we scrape [this website](http://www.nrc.gov/reactors/operating/list-power-reactor-units.html)?\n",
    "\n",
    "1. We will import some libraries that:\n",
    "    - Act like an internet browser\n",
    "    - Parse HTML code\n",
    "    - Read and write CSV files\n",
    "2. Grab the contents of the web page.\n",
    "3. Parse the contents of the web page and target only the data table.\n",
    "4. Open a blank CSV file to store the information in the data table.\n",
    "5. Loop through each row in the online data table:\n",
    "    - Extract each element (cell) and store it in a variable\n",
    "    - Write those variables as a row into the CSV file\n",
    "6. Close the CSV file.\n",
    "7. Rejoice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why will we scrape this way?\n",
    "\n",
    "While code-free tools are handy in a pinch, scripts written in Python or another language are more flexible and adaptable. They can also run automatically in the background on a schedule. Also, you don't have to worry about a service or a tool ever disappearing, making all your hard work for naught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries to do the heavy lifting\n",
    "\n",
    "We're going to bring in three outside modules to help us scrape this page.\n",
    "\n",
    "- **requests** will act like an internet browser and collect HTML\n",
    "- **BeautifulSoup** will parse the HTML code and allow us to isolate a data table\n",
    "- **csv** will allow us to write what we find to a nicely formatted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Grab the contents of a web page.\n",
    "\n",
    "The page we want is located here: http://www.nrc.gov/reactors/operating/list-power-reactor-units.html\n",
    "\n",
    "**requests** has a method called *get*, which is analagous to a browser like Firefox or Chrome fetching the HTML code for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.nrc.gov/reactors/operating/list-power-reactor-units.html\"\n",
    "web_page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this quickly to see if we've gotten the expected raw HTML code by using another **requests** method that returns the HTML code as plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(web_page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parse the HTML and target the table\n",
    "\n",
    "Now we can send our HTML code to **BeautifulSoup**, which is specifically designed to navigate the structural elements of the document, breaking off the pieces we choose. In this case, we are after the web page's only table -- it has all the data we need.\n",
    "\n",
    "**BeautifulSoup** has methods called *find* and *find_all* designed to target HTML tags. While *find* picks up the first matching instance, *find_all* locates all matching instances and returns them as a kind of list. We will use this to our advantage in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(web_page.content, 'html.parser')\n",
    "reactor_table = soup.find('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can check to see if we've isolated the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(reactor_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Open a blank CSV file for data storage\n",
    "\n",
    "We need a place for all this data to go once we start scraping it; we can open a new blank file and then use the **csv** method *writer* to create an object (stay with me now) that we can order around with some basic commands, making it write data to the new blank file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = open('reactors.csv', 'wb')\n",
    "output = csv.writer(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our inaugural row to the file: the header that specifies what all the different columns are. We'll use **csv**'s *writerow* to send a list of what we would like written to the file: `\"NAME\", \"LINK\", \"DOCKET\", \"LICENSE_NUM\", \"TYPE\", \"LOCATION\", \"OWNER\", \"REGION\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.writerow([\"NAME\", \"LINK\", \"DOCKET\", \"LICENSE_NUM\", \"TYPE\", \"LOCATION\", \"OWNER\", \"REGION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loop through each row in the table, extract data and write it to the file\n",
    "\n",
    "Here comes the tricky part: we have to actually scrape the data out of the table we isolated.\n",
    "\n",
    "To do that, we need to not only loop through every row in the table, but also each cell in every row.\n",
    "\n",
    "Remember, if I want to do something to each item in this list without having to retype it repeatedly, this basic syntax, in pseudocode:\n",
    "\n",
    "```\n",
    "for [a list item] in [some list]:\n",
    "    do a thing with [a list item]\n",
    "```\n",
    "\n",
    "That thing will then happen with the first list item, the second, the third, etc., until the end of the list is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab one row from this table to see what we might have to do in order to extract the text from each cell into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td scope=\"row\"><a href=\"/info-finder/reactors/ano1.html\">Arkansas Nuclear 1</a><br/>\\r\\n            05000313</td>, <td align=\"center\">DPR-51</td>, <td>PWR</td>, <td>6 miles WNW of Russellville,\\xa0\\xa0AR</td>, <td>Entergy Nuclear Operations, Inc. </td>, <td align=\"middle\">4</td>]\n"
     ]
    }
   ],
   "source": [
    "test_row = reactor_table.find_all('tr')[1]\n",
    "cell_list = test_row.find_all('td')\n",
    "\n",
    "print(cell_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hone in on that first cell containing a reactor name, a docket number and a partial URL leading to the reactor page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/info-finder/reactors/ano1.html\">Arkansas Nuclear 1</a>,\n",
       " <br/>,\n",
       " u'\\r\\n            05000313']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_list[0].contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we pick out the text components and the URL from the contents? By using `BeautifulSoup`'s `.text` method to isolate text within HTML tags and `.get` to slice out the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arkansas Nuclear 1\n",
      "/info-finder/reactors/ano1.html\n",
      "05000313\n"
     ]
    }
   ],
   "source": [
    "print(cell_list[0].contents[0].text)\n",
    "print(cell_list[0].contents[0].get('href'))\n",
    "print(cell_list[0].contents[2].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, you should kind of get the idea now about what the process will be like to dive into other cells in this row. Instead of extracting information from `cell_list[0]`, we'll be going into `cell_list[1]`, `cell_list[2]`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we should be able to dive into the table with this long-ish list of things to grab and pass into variables. We'll make a list of HTML snippets wrapped in `<tr>` tags (the table rows), and then a list within that of the actual data cells inside each `<td>`. We'll crawl through those, extracting data and passing it to variables. At the end of each iteration, we'll write the row to the output file; it will then start all over again with the next row.\n",
    "\n",
    "**One point of weirdness**: This webpage is encoded in UTF-8, meaning it has the ability to have characters that fall outside the western ASCII set. Python 2 doesn't like this. There are some characters that aren't part of ASCII in the location and owner columns, so we'll have to encode them before they are written to the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in reactor_table.find_all('tr')[1:]:\n",
    "    \n",
    "    cell = row.find_all('td')\n",
    "    \n",
    "    name = cell[0].contents[0].text\n",
    "    link = cell[0].contents[0].get('href')\n",
    "    docket = cell[0].contents[2].strip()\n",
    "    lic_num = cell[1].text\n",
    "    reactype = cell[2].text\n",
    "    \n",
    "    location = cell[3].text.encode('utf-8')\n",
    "    owner = cell[4].text.strip().encode('utf-8')\n",
    "    region = cell[5].text\n",
    "\n",
    "    output.writerow([name, link, docket, lic_num, reactype, location, owner, region])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop has done all the work! Just one thing left to do:\n",
    "\n",
    "### 6. Close the file\n",
    "\n",
    "Some of it just hangs out in the computer's memory until you close the file and commit it all to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
